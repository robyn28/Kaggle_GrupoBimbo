{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03-exploratory-analysis for final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am working on the Kaggle Grupo Bimbo competition dataset for this project. \n",
    "Link to Grupo Bimbo Kaggle competition: [Kaggle-GrupoBimbo](https://www.kaggle.com/c/grupo-bimbo-inventory-demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\", font_scale=1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/workflow/data-science-workflow-01.png)\n",
    "\n",
    "## Part 1. Identify the Problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Given various sales/client/product data, we want to predict demand for each product at each store on a weekly basis. Per the train dataset, the average demand for a product at a store per week is 7.2 units. However, this does not factor in cases in which store managers under-predict demand for a product which we can see when returns=0 for that week. There are 74,180,464 records in the train data, of which 71,636,003 records have returns=0 or approx 96%. This generally means that managers probably often under predict product demand (unless that are exact on the money, which seems unlikely). \n",
    "\n",
    "**Goals**: The goal is to predict demand for each product at each store on a weekly basis while avoiding under-predicting demand.\n",
    "\n",
    "**Hypothesis**: As stated previously, the average product demand at a store per week is 7.2 units per the train data. However, given the likelihood of managers underpredicint product demand, I hypothesize a good model should return a number higher than 7.2 units to more accurately predict demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/workflow/data-science-workflow-02.png)\n",
    "\n",
    "## Part 2. Acquire the Data\n",
    "\n",
    "Kaggle has provided five files for this dataset:  \n",
    "_train.csv_: Use for building a model (contains target variable \"Demanda_uni_equil\")  \n",
    "_test.csv_: Use for submission file (fill in for target variable \"Demanda_uni_equil\")\n",
    "_cliente_tabla.csv_: Contains client names (can be joined with train/test on Cliente_ID)\n",
    "_producto_tabla.csv_: Contains product names (can be join with train/test on Producto_ID)\n",
    "_town_state.csv_: Contains town and state (can be join with train/test on Agencia_ID)\n",
    "\n",
    "\n",
    "**Notes**: I will further split _train.csv_ to generate my own cross validation set. However, I will use all of _train.csv_ to train my final model since Kaggle has already supplied a test dataset. Additionally, I am only using a random 10% of the train data given to me for EDA and model development. Using the entire train dataset proved to be too time consuming for the quick iternations needed for initial modeling building and EDA efforts. I plan to use 100% of the train dataset once I build a model I'm comfortable with. I may have to explore using EC2 for this effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load train data\n",
    "# Given size of training data, I chose to use only 10% for speed reasons\n",
    "# QUESTION - how can i randomize with python? i used sql to create the random sample below.\n",
    "df_train = pd.read_csv(\"train_random10percent.csv\")\n",
    "\n",
    "# Check head\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Check head. I noticed that I will have to drop certain columns so that test and train sets have the same features. \n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#given that i cannot use a significant amount of variables in train data, i created additoinal features using the mean\n",
    "#i grouped on product id since i will ultimately be predicting demand for each product\n",
    "\n",
    "df_train_mean = df_train.groupby('Producto_ID').mean().add_suffix('_mean').reset_index()\n",
    "df_train_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from above, adding 2 additional features, the average sales units and the average demand\n",
    "df_train2 = df_train.merge(df_train_mean[['Producto_ID','Venta_uni_hoy_mean', 'Demanda_uni_equil_mean']],how='inner',on='Producto_ID')\n",
    "df_train2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding features to the test set in order to match train set\n",
    "df_test2 = df_test.merge(df_train_mean[['Producto_ID','Venta_uni_hoy_mean', 'Demanda_uni_equil_mean']],how='left',on='Producto_ID')\n",
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/workflow/data-science-workflow-03-05.png)\n",
    "\n",
    "## Part 3. Parse, Mine, and Refine the data\n",
    "\n",
    "Perform exploratory data analysis and verify the quality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check columns and counts to drop any non-generic or near-empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check columns\n",
    "print \"train dataset columns:\"\n",
    "print df_train2.columns.values\n",
    "print \n",
    "print \"test dataset columns:\"\n",
    "print df_test2.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check counts\n",
    "print \"train dataset counts:\"\n",
    "print df_train2.count()\n",
    "print\n",
    "print \"test dataset counts:\"\n",
    "print df_test2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values and drop or impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check counts for missing values in each column\n",
    "print \"train dataset missing values:\"\n",
    "print df_train2.isnull().sum()\n",
    "print\n",
    "print \"test dataset missing values:\"\n",
    "print df_test2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle the data to address any issues from above checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop columns not included in test dataset\n",
    "df_train2 = df_train2.drop(['Venta_uni_hoy', 'Venta_hoy', 'Dev_uni_proxima', 'Dev_proxima'], axis=1)\n",
    "\n",
    "# Check data\n",
    "df_train2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop blank values in test set and replace with mean\n",
    "\n",
    "# Replace missing values for venta_uni_hoy_mean using mean\n",
    "df_test2.loc[(df_test2['Venta_uni_hoy_mean'].isnull()), 'Venta_uni_hoy_mean'] = df_test2['Venta_uni_hoy_mean'].dropna().mean()\n",
    "\n",
    "# Replace missing values for demand using mean\n",
    "df_test2.loc[(df_test2['Demanda_uni_equil_mean'].isnull()), 'Demanda_uni_equil_mean'] = df_test2['Demanda_uni_equil_mean'].dropna().mean()\n",
    "\n",
    "print \"test dataset missing values:\"\n",
    "print df_test2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get summary statistics for data\n",
    "df_train2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#RE RUN THIS LAST\n",
    "# Get pair plot for data\n",
    "sns.pairplot(df_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show demand by weeks\n",
    "timing = pd.read_csv('train_random10percent.csv', usecols=['Semana','Demanda_uni_equil'])\n",
    "print(timing['Semana'].value_counts())\n",
    "plt.hist(timing['Semana'].tolist(), bins=7, color='blue')\n",
    "plt.show()\n",
    "\n",
    "#QUESTION - is this a time series problem since we are predicting demand for weeks 10 and 11? and beyond?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Show box plot of demand by week\n",
    "sns.factorplot(\n",
    "    x='Semana',\n",
    "    y='Demanda_uni_equil',\n",
    "    data=df_train2,\n",
    "    kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and convert all data types to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check data types\n",
    "df_train.dtypes\n",
    "\n",
    "#these are all numerical but are not continuous values and therefore don't have relative significant to one another, except for week\n",
    "#however, creating dummy variables for all these is too memory intensive. as such, might have to explore using a random forest model\n",
    "#in addition to the linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/workflow/data-science-workflow-06.png)\n",
    "\n",
    "## Part 4. Build a Model\n",
    "\n",
    "Create a cross validation split, select and build a model, evaluate the model, and refine the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cross validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create cross validation sets\n",
    "\n",
    "#set target variable name\n",
    "target = 'Demanda_uni_equil'\n",
    "\n",
    "#set X and y\n",
    "X = df_train2.drop([target], axis=1)\n",
    "y = df_train2[target]\n",
    "\n",
    "# create separate training and test sets with 60/40 train/test split\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size= .4, random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create linear regression object\n",
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "#train the model using the training data\n",
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check R^2 on test set\n",
    "print \"R^2: %0.3f\" % lm.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check MSE on test set\n",
    "#http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "print \"MSE: %0.3f\" % metrics.mean_squared_error(y_test, lm.predict(X_test))\n",
    "#QUESTION - should i check this on train set?\n",
    "print \"MSE: %0.3f\" % metrics.mean_squared_error(y_train, lm.predict(X_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/workflow/data-science-workflow-07.png)\n",
    "\n",
    "## Part 5: Present the Results\n",
    "\n",
    "Generate summary of findings and kaggle submission file.\n",
    "\n",
    "NOTE: For the purposes of generating summary narratives and kaggle submission, we can train the model on the entire training data provided in _train.csv_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Kaggle training data and use entire data to train tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set target variable name\n",
    "target = 'Demanda_uni_equil'\n",
    "\n",
    "# Set X_train and y_train\n",
    "X_train = df_train2.drop([target], axis=1)\n",
    "y_train = df_train2[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build tuned model\n",
    "#create linear regression object\n",
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "#train the model using the training data\n",
    "lm.fit(X_train,y_train)\n",
    "\n",
    "# Score tuned model\n",
    "print \"R^2: %0.3f\" % lm.score(X_train, y_train)\n",
    "print \"MSE: %0.3f\" % metrics.mean_squared_error(y_train, lm.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Kaggle test data, make predictions using model, and generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create data frame for submission\n",
    "df_sub = df_test2[['id']]\n",
    "\n",
    "df_test2 = df_test2.drop('id', axis=1)\n",
    "\n",
    "#predict using tuned model\n",
    "df_sub['Demanda_uni_equil'] = lm.predict(df_test2)\n",
    "\n",
    "df_sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = df_sub['Demanda_uni_equil']\n",
    "d[d<0] = 0\n",
    "df_sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write submission file\n",
    "df_sub.to_csv(\"mysubmission3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Kaggle score** : 0.75682\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#notes\n",
    "#want to try to use a classifier like random forest or logistic regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
